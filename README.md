# Stratified-LLM-Subsets-Pre-Training-Instruction-Following-and-Reasoning-SFT-data-at-100K-1M-Scale
Stratified LLM Subsets delivers balanced training data at 100K-1M scales across pre-training (FineWeb-Edu, Proof-Pile-2), instruction-following (Tulu-3, Orca AgentInstruct), and reasoning distillation (Nemotron, AM-DeepSeek). Square-root balancing prevents category dominance while maintaining diversity across 6 high-quality open datasets.
